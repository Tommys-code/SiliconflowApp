[
  {
    "model": "Qwen/Qwen3-8B",
    "manu": "Qwen3",
    "per_cost": 0,
    "max_tokens": 8192,
    "enable_thinking": true,
    "desc": "Qwen3-8B 是通义千问系列的最新大语言模型，拥有 8.2B 参数量。该模型独特地支持在思考模式（适用于复杂逻辑推理、数学和编程）和非思考模式（适用于高效的通用对话）之间无缝切换，显著增强了推理能力。模型在数学、代码生成和常识逻辑推理上表现优异，并在创意写作、角色扮演和多轮对话等方面展现出卓越的人类偏好对齐能力。此外，该模型支持 100 多种语言和方言，具备出色的多语言指令遵循和翻译能力"
  },
  {
    "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    "manu": "DeepSeek",
    "per_cost": 0,
    "max_tokens": 16384,
    "desc": "DeepSeek-R1-Distill-Qwen-7B 是基于 Qwen2.5-Math-7B 通过知识蒸馏得到的模型。该模型使用 DeepSeek-R1 生成的 80 万个精选样本进行微调，展现出优秀的推理能力。在多个基准测试中表现出色，其中在 MATH-500 上达到了 92.8% 的准确率，在 AIME 2024 上达到了 55.5% 的通过率，在 CodeForces 上获得了 1189 的评分，作为 7B 规模的模型展示了较强的数学和编程能力"
  },
  {
    "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
    "manu": "DeepSeek",
    "per_cost": 0,
    "max_tokens": 16384,
    "desc": "DeepSeek-R1-Distill-Qwen-1.5B 是基于 Qwen2.5-Math-1.5B 通过知识蒸馏得到的模型。该模型使用 DeepSeek-R1 生成的 80 万个精选样本进行微调，在多个基准测试中展现出不错的性能。作为一个轻量级模型，在 MATH-500 上达到了 83.9% 的准确率，在 AIME 2024 上达到了 28.9% 的通过率，在 CodeForces 上获得了 954 的评分，显示出超出其参数规模的推理能力"
  },
  {
    "model": "Qwen/Qwen2.5-7B-Instruct",
    "manu": "Qwen2.5",
    "per_cost": 0,
    "max_tokens": 4096,
    "desc": "Qwen2.5-7B-Instruct 是阿里云发布的最新大语言模型系列之一。该 7B 模型在编码和数学等领域具有显著改进的能力。该模型还提供了多语言支持，覆盖超过 29 种语言，包括中文、英文等。模型在指令跟随、理解结构化数据以及生成结构化输出（尤其是 JSON）方面都有显著提升"
  },
  {
    "model": "Qwen/Qwen2.5-Coder-7B-Instruct",
    "manu": "Qwen2.5",
    "per_cost": 0,
    "max_tokens": 4096,
    "desc": "Qwen2.5-Coder-7B-Instruct 是阿里云发布的代码特定大语言模型系列的最新版本。该模型在 Qwen2.5 的基础上，通过 5.5 万亿个 tokens 的训练，显著提升了代码生成、推理和修复能力。它不仅增强了编码能力，还保持了数学和通用能力的优势。模型为代码智能体等实际应用提供了更全面的基础"
  },
  {
    "model": "internlm/internlm2_5-7b-chat",
    "manu": "书生·浦语",
    "per_cost": 0,
    "max_tokens": 4096,
    "desc": "InternLM2.5-7B-Chat 是一个开源的对话模型，基于 InternLM2 架构开发。该 7B 参数规模的模型专注于对话生成任务，支持中英双语交互。模型采用了最新的训练技术，旨在提供流畅、智能的对话体验。InternLM2.5-7B-Chat 适用于各种对话应用场景，包括但不限于智能客服、个人助手等领域"
  },
  {
    "model": "Qwen/Qwen2-7B-Instruct",
    "manu": "Qwen2",
    "per_cost": 0,
    "max_tokens": 4096,
    "desc": "Qwen2-7B-Instruct 是 Qwen2 系列中的指令微调大语言模型，参数规模为 7B。该模型基于 Transformer 架构，采用了 SwiGLU 激活函数、注意力 QKV 偏置和组查询注意力等技术。它能够处理大规模输入。该模型在语言理解、生成、多语言能力、编码、数学和推理等多个基准测试中表现出色，超越了大多数开源模型，并在某些任务上展现出与专有模型相当的竞争力。Qwen2-7B-Instruct 在多项评测中均优于 Qwen1.5-7B-Chat，显示出显著的性能提升"
  },
  {
    "model": "Qwen/Qwen2-1.5B-Instruct",
    "manu": "Qwen2",
    "per_cost": 0,
    "max_tokens": 4096,
    "desc": "Qwen2-1.5B-Instruct 是 Qwen2 系列中的指令微调大语言模型，参数规模为 1.5B。该模型基于 Transformer 架构，采用了 SwiGLU 激活函数、注意力 QKV 偏置和组查询注意力等技术。它在语言理解、生成、多语言能力、编码、数学和推理等多个基准测试中表现出色，超越了大多数开源模型。与 Qwen1.5-1.8B-Chat 相比，Qwen2-1.5B-Instruct 在 MMLU、HumanEval、GSM8K、C-Eval 和 IFEval 等测试中均显示出显著的性能提升，尽管参数量略少"
  },
  {
    "model": "THUDM/GLM-Z1-9B-0414",
    "manu": "智谱AI",
    "per_cost": 0,
    "max_tokens": 8192,
    "desc": "GLM-Z1-9B-0414 是 GLM 系列的小型模型，仅有 90 亿参数，但保持了开源传统的同时展现出惊人的能力。尽管规模较小，该模型在数学推理和通用任务上仍表现出色，其总体性能在同等规模的开源模型中已处于领先水平。研究团队采用了与大模型相同的一系列技术进行训练，使其在资源受限的场景中能够实现效率与效果的绝佳平衡，为寻求轻量级部署的用户提供强大选择。特别是在资源受限的场景下，该模型可以很好地在效率与效果之间取得平衡，为需要轻量化部署的用户提供强有力的选择"
  },
  {
    "model": "THUDM/GLM-4-9B-0414",
    "manu": "智谱AI",
    "per_cost": 0,
    "max_tokens": 8192,
    "desc": "GLM-4-9B-0414 是 GLM 系列的小型模型，拥有 90 亿参数。该模型继承了 GLM-4-32B 系列的技术特点，但提供了更轻量级的部署选择。尽管规模较小，GLM-4-9B-0414 仍在代码生成、网页设计、SVG 图形生成和基于搜索的写作等任务上展现出色能力。该模型还支持函数调用功能，可以调用外部工具以扩展其能力范围。模型在资源受限的场景中表现出良好的效率与效果平衡，为需要在计算资源有限条件下部署 AI 模型的用户提供了强大选择。与其他同系列模型一样，GLM-4-9B-0414 也展示了在各种基准测试中的竞争性能力"
  },
  {
    "model": "THUDM/glm-4-9b-chat",
    "manu": "智谱AI",
    "per_cost": 0,
    "max_tokens": 4096,
    "desc": "GLM-4-9B-Chat 是智谱 AI 推出的 GLM-4 系列预训练模型中的开源版本。该模型在语义、数学、推理、代码和知识等多个方面表现出色。除了支持多轮对话外，GLM-4-9B-Chat 还具备网页浏览、代码执行、自定义工具调用（Function Call）和长文本推理等高级功能。模型支持 26 种语言，包括中文、英文、日语、韩语和德语等。在多项基准测试中，GLM-4-9B-Chat 展现了优秀的性能，如 AlignBench-v2、MT-Bench、MMLU 和 C-Eval 等。该模型支持最大 128K 的上下文长度，适用于学术研究和商业应用"
  },
  {
    "model": "THUDM/chatglm3-6b",
    "manu": "智谱AI",
    "per_cost": 0,
    "max_tokens": 4096,
    "desc": "ChatGLM3-6B 是 ChatGLM 系列的开源模型，由智谱 AI 开发。该模型保留了前代模型的优秀特性，如对话流畅和部署门槛低，同时引入了新的特性。它采用了更多样的训练数据、更充分的训练步数和更合理的训练策略，在 10B 以下的预训练模型中表现出色。ChatGLM3-6B 支持多轮对话、工具调用、代码执行和 Agent 任务等复杂场景。除对话模型外，还开源了基础模型 ChatGLM-6B-Base 和长文本对话模型 ChatGLM3-6B-32K。该模型对学术研究完全开放，在登记后也允许免费商业使用"
  }
]